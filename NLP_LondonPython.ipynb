{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_LondonPython.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CelineBoudier/google-colab-notebooks/blob/main/NLP_LondonPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBB6SIcIJl68"
      },
      "source": [
        "#**LondonPython Nov 2019 - code snippets**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFGea2dmJxWr"
      },
      "source": [
        "Trigrams, custom\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT4L3LM6e6Bv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1319791c-771a-4f5b-f3a0-a46e707be945"
      },
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.util import trigrams\n",
        "\n",
        "from nltk.corpus import udhr\n",
        "nltk.download('udhr')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/udhr.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwdGOpcQeFEy"
      },
      "source": [
        "udhr.fileids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EJdoBa_pABb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1a3eaf51-5bc0-4f44-cf24-c73f94f25e9d"
      },
      "source": [
        "\n",
        "languages = [('English', 'Latin1'), ('Polish','Latin2'), \n",
        "             ('French_Francais', 'Latin1'), ('Russian', 'UTF8'), \n",
        "             ('Mongolian_Khalkha', 'UTF8')]\n",
        "\n",
        "def get_trigrams(words):\n",
        "  text_trigrams = []\n",
        "  for word in words:\n",
        "    text_trigrams.extend(get_words_trigrams(word))\n",
        "  return dict(Counter(text_trigrams))\n",
        "\n",
        "def get_words_trigrams(word):\n",
        "  if len(word)>2:\n",
        "    return list(trigrams(word))\n",
        "  else:\n",
        "    return [tuple(word)]\n",
        "    \n",
        "def create_corpus_data(languages):\n",
        "  data_corpus = {}\n",
        "  for lang, enc in languages:\n",
        "    data_corpus[lang] = get_trigrams(udhr.words(f\"{lang}-{enc}\"))\n",
        "  return data_corpus\n",
        "\n",
        "def compute_language_probs(text, data):\n",
        "    text_trigrams = get_trigrams(text.split())\n",
        "    trigrams_number = sum(text_trigrams.values())\n",
        "    probas = {}\n",
        "    for language, trigram_counter in data.items():\n",
        "        prob = 0.0\n",
        "        corpus_trigrams_number = sum(trigram_counter.values())\n",
        "        for k, v in text_trigrams.items():\n",
        "            try:\n",
        "                freq = float(trigram_counter[k])\n",
        "            except KeyError:\n",
        "                freq = 0.0\n",
        "            prob += (freq/float(corpus_trigrams_number)) * (float(v)/ float(trigrams_number))\n",
        "        probas[language]=prob\n",
        "    return(Counter(probas))\n",
        "  \n",
        "    \n",
        "print(compute_language_probs(\"\", create_corpus_data(languages)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'English': 0.0, 'Polish': 0.0, 'French_Francais': 0.0, 'Russian': 0.0, 'Mongolian_Khalkha': 0.0})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeK3FVcmJ1lf"
      },
      "source": [
        "language detection, tools\n",
        "\n",
        "*  https://pypi.org/project/langdetect/\n",
        "*  https://github.com/saffsd/langid.py \n",
        "*  fasttext\n",
        "*  nltk\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "You need to upload a corpus like lid.176.ftz, available here https://fasttext.cc/docs/en/language-identification.html\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PaYjoORNPts",
        "outputId": "0051ddda-fd77-43ff-f362-499bf1002748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        }
      },
      "source": [
        "!pip install -U regex\n",
        "!pip install -U langdetect\n",
        "!pip install -U langid\n",
        "!pip install -U pyfasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\r\u001b[K     |▌                               | 10kB 15.4MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 4.7MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 4.8MB/s \n",
            "\u001b[?25hInstalling collected packages: regex\n",
            "Successfully installed regex-2019.11.1\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=147349ec899fa181f121840ca8e25ae13ae0fbca268b079ca68be25213fb46c0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.7\n",
            "Collecting langid\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/4c/0fb7d900d3b0b9c8703be316fbddffecdab23c64e1b46c7a83561d78bd43/langid-1.1.6.tar.gz (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from langid) (1.17.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-cp36-none-any.whl size=1941190 sha256=c5cb662873c2f2a587fd7900a7e1809bccfa3d733f23e5944be93a69b051d3ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/bc/61/50a93be85d1afe9436c3dc61f38da8ad7b637a38af4824e86e\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n",
            "Collecting pyfasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/ef/90606442481d1e4ab10eba8c2b2c449ceaa70c60e9b8d5898bb7504e3634/pyfasttext-0.4.6.tar.gz (244kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyfasttext) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: cysignals in /usr/local/lib/python3.6/dist-packages (from pyfasttext) (1.10.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from pyfasttext) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: Cython>=0.28 in /usr/local/lib/python3.6/dist-packages (from cysignals->pyfasttext) (0.29.14)\n",
            "Building wheels for collected packages: pyfasttext\n",
            "  Building wheel for pyfasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfasttext: filename=pyfasttext-0.4.6-cp36-cp36m-linux_x86_64.whl size=1473945 sha256=b8d8afc6fab0540d0fa5328fb6d48934abad750105b01604efed8fb2cacdb5ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/de/c6/3d26a304c069689a7bf5ef2cc774588663700c8381dbf3d947\n",
            "Successfully built pyfasttext\n",
            "Installing collected packages: pyfasttext\n",
            "Successfully installed pyfasttext-0.4.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdoyS3S8fC5K",
        "outputId": "a24c6aff-5fc9-4311-99c7-9226f627a413",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-11b2e88d-111d-445a-863a-fdf0ffe55f6c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-11b2e88d-111d-445a-863a-fdf0ffe55f6c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving lid.176.ftz to lid.176.ftz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPiS6aSGJ3Kw",
        "outputId": "8066bc6b-8683-4dd2-aef6-0874f7062ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "sentences = [\"Twas brillig, and the slithy toves.\",\n",
        "             \"Je vais à un concert de Nightwish.\",\n",
        "             \"Ostatnia powieść Olgi Tokarczuk odniosła sukces...\",\n",
        "             \"Wow!\",\n",
        "             \"To jest\"]\n",
        "from langdetect import detect\n",
        "\n",
        "from langid import classify as langid_classify\n",
        "\n",
        "import nltk\n",
        "nltk.download('crubadan')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from pyfasttext import FastText\n",
        "model = FastText('lid.176.ftz')\n",
        "\n",
        "for sentence in sentences:\n",
        "  print (sentence)\n",
        "  print('Langid:')\n",
        "  print(langid_classify(sentence))\n",
        "  print('langdetect:')\n",
        "  print(detect(sentence))\n",
        "  #print('textcat:')\n",
        "  #print(nltk.classify.textcat.TextCat().guess_language(sentence))\n",
        "  print('fasttext:')\n",
        "  print(model.predict_proba_single(sentence, k=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Twas brillig, and the slithy toves.\n",
            "Langid:\n",
            "('en', -95.49024057388306)\n",
            "langdetect:\n",
            "en\n",
            "fasttext:\n",
            "[('en', 0.6093597357505774)]\n",
            "Je vais à un concert de Nightwish.\n",
            "Langid:\n",
            "('fr', -90.57910108566284)\n",
            "langdetect:\n",
            "fr\n",
            "fasttext:\n",
            "[('fr', 0.9728845225614599)]\n",
            "Ostatnia powieść Olgi Tokarczuk odniosła sukces...\n",
            "Langid:\n",
            "('pl', -84.67223596572876)\n",
            "langdetect:\n",
            "pl\n",
            "fasttext:\n",
            "[('pl', 0.978671573701704)]\n",
            "Wow!\n",
            "Langid:\n",
            "('en', 9.061840057373047)\n",
            "langdetect:\n",
            "pl\n",
            "fasttext:\n",
            "[('en', 0.8432655354756698)]\n",
            "To jest\n",
            "Langid:\n",
            "('pl', -10.821922779083252)\n",
            "langdetect:\n",
            "hr\n",
            "fasttext:\n",
            "[('pl', 1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmL7gR4HPn1x"
      },
      "source": [
        "Markov chains, custom\n",
        "\n",
        "\n",
        "---\n",
        "You need to upload some texts in .txt format in one language, like some novels. Ex: http://www.gutenberg.org/ebooks/author/32429\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFRo1ZnjekDo"
      },
      "source": [
        "#from google.colab import files\n",
        "\n",
        "#uploaded = files.upload()\n",
        "\n",
        "def generate_triples(text, gen_words=True):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    if gen_words:\n",
        "      words = text.split()\n",
        "    else:\n",
        "      words = [text[i:i+3] for i in range(len(text)-3)]\n",
        "    triples = []\n",
        "    if len(words) >=3:\n",
        "      for i in range(len(words) -2):\n",
        "        triples.append((words[i].replace(\"\\'\", \"'\").replace(\"\\\\\", \"\"), words[i + 1].replace(\"\\'\", \"'\").replace(\"\\\\\", \"\"), words[i + 2].replace(\"\\'\", \"'\").replace(\"\\\\\", \"\")))\n",
        "    return triples\n",
        "      \n",
        "def generate_data(feed_text, gen_words=True):\n",
        "    triples = generate_triples(feed_text, gen_words)\n",
        "    data = {}\n",
        "    for a,b,c in triples:\n",
        "      try:\n",
        "        data[(a, b)].append(c)\n",
        "      except:\n",
        "        data[(a, b)] = [c]\n",
        "    return data   \n",
        "\n",
        "language_data = [(\"Polish\", \"mickiewicz.txt\"), (\"French\", \"segur.txt\"), \n",
        "                 (\"English\", \"shelley.txt\"), (\"Mandarin Chinese\", \"buddha.txt\"),\n",
        "                 (\"Drunk Celine\", \"text.txt\")]\n",
        "datas = {}\n",
        "for item in language_data:\n",
        "  with open(item[1]) as feed_file:\n",
        "    feed_text = feed_file.read()\n",
        "  datas[item[0]] = generate_data(feed_text, True)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIXcGW3FUBv6"
      },
      "source": [
        "import random\n",
        "data = datas[\"Drunk Celine\"]\n",
        "beginners = [i for i in data.keys() if i[0][0].isupper()]\n",
        "#beginners = [i for i in data.keys() ]\n",
        "\n",
        "def generate_sentence(min_number_words, gen_words=True):\n",
        "  choose_tuple = random.choice(beginners)\n",
        "  sentence_1 = choose_tuple[0]\n",
        "  sentence_2 = choose_tuple[1]\n",
        "  sentence_3 = random.choice(data[choose_tuple])\n",
        "  if gen_words:\n",
        "    separator = \" \"\n",
        "    sentence_elems = [sentence_1, sentence_2, sentence_3]\n",
        "  else: \n",
        "    separator = \"\"\n",
        "    sentence_elems = [sentence_1, sentence_2[-1], sentence_3[-1]]\n",
        "  while (len(sentence_elems) < min_number_words) or ((len(sentence_elems) >= min_number_words) and not sentence_3.endswith(\".\")and not sentence_3.endswith(\"!\")and not sentence_3.endswith(\"?\")and not sentence_3.endswith(\"。\")):\n",
        "    sentence_1, sentence_2 = sentence_2, sentence_3\n",
        "    sentence_3 = random.choice(data[(sentence_1, sentence_2)])\n",
        "    if gen_words:\n",
        "      sentence_elems.append(sentence_3)\n",
        "    else:\n",
        "      sentence_elems.append(sentence_3[-1])\n",
        "  generated_sentence = separator.join(sentence_elems)\n",
        "  return generated_sentence\n",
        "\n",
        "\n",
        "generate_sentence(2, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT6S4zM3mEHy"
      },
      "source": [
        "Examples:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "'I? I had brought a few badly lit corridors and waited for a while, until I realized hours had slipped by as it was clearly harmless, not like clothing moths or other bugs.'\n",
        "I let the stations flow past in a modernist museum.\n",
        "Curly, long fingertips brush on silky skin while a shark takes its toll on a derelict path, waiting for the train that would lead me to enter.\n",
        "We danced and we are still the only one to live this incredible experience.\n",
        "'I dziecię bierze do ręki, U łona białego tuli, „Luli, woła, mój maleńki!'\n",
        "'C'était le 19 juillet, jour de la diligence; le paquet sur la chaux, pensant que c'était la mort de son couteau, s'en servait pour nettoyer l'argenterie; elle en prit un, qu'elle cacha dans le tien; ma bonne l'a recouverte de percale rose; c'est très joli; venez voir.'\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTZ0Ce3lm0HZ"
      },
      "source": [
        "get antonyms\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un1VMzrJmykW"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "conv_pos = {'NOUN':wordnet.NOUN, 'PROPN':wordnet.NOUN, 'ADJ':wordnet.ADJ,'VERB':wordnet.VERB,'ADV':wordnet.ADV}\n",
        "\n",
        "def find_antonym(word, pos):\n",
        "  if pos in conv_pos:\n",
        "    for syn in wordnet.synsets(word, pos=conv_pos[pos]):\n",
        "      for lemma in syn.lemmas():\n",
        "        if lemma.antonyms():\n",
        "          return lemma.antonyms()[0].name()\n",
        "  return word\n",
        "\n",
        "print(find_antonym(\"death\", \"NOUN\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T3QWM6qrQ8R"
      },
      "source": [
        "POS and NER\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oFvUfulrmbj"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk\n",
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "texts= [\"This is a love song by Queen from the UK.\", \n",
        "        \"Hello, London!\",\n",
        "        \"Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.\",\n",
        "        \"Time flies like an arrow.\",\n",
        "        \"Fruit flies like a banana.\"]\n",
        "        \n",
        "for text in texts:\n",
        "  tokens_spacy = nlp(text)\n",
        "  print(text)\n",
        "  print(\"\\n\")\n",
        "  nltk_tok = word_tokenize(text)\n",
        "  tokens_nltk = nltk.pos_tag(nltk_tok)\n",
        "  \n",
        "  print(\"spaCy\")\n",
        "  print(\"POS\")\n",
        "  for token in tokens_spacy:\n",
        "    print(token.text,token.pos_, token.tag_)\n",
        "    \n",
        "  print(\"\\n\")\n",
        "  print(\"NER\")\n",
        "  for ent in tokens_spacy.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "    \n",
        "  print(\"\\nnltk\")\n",
        "  print(ne_chunk(tokens_nltk))\n",
        "  \n",
        "  print(\"\\n\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UFvAR_mwAwY"
      },
      "source": [
        "sentence stemmer\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEcVjRs7zowE"
      },
      "source": [
        "\n",
        "\n",
        "def stem_word(sentence):\n",
        "  phrase = nlp(sentence)\n",
        "  words = []\n",
        "  for token in phrase:\n",
        "    if token.dep_ not in [\"aux\", \"det\"]:\n",
        "      words.append(token.text)\n",
        "  return \" \".join(words)\n",
        "\n",
        "\n",
        "print(stem_word(\"a small and thinly populated rural area\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PNMK0AisO9j"
      },
      "source": [
        "antonimiser - spaCy - loop\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY0_rJKdsX3M"
      },
      "source": [
        "depth = 3\n",
        "        \n",
        "        \n",
        "def rec_find_antonym(word, pos, depth=1):\n",
        "        antonym = find_antonym(word, pos)\n",
        "        if antonym != word:\n",
        "          return antonym\n",
        "        if depth<=1:\n",
        "            return word\n",
        "        else:\n",
        "            definition = nlp(stem_word(find_definition(word, pos)))\n",
        "            return \" \".join([rec_find_antonym(token.text, token.pos_, depth-1) for token in definition])\n",
        "          \n",
        "def find_definition(word, pos):\n",
        "  if pos in conv_pos:\n",
        "    for syn in wordnet.synsets(word, pos=conv_pos[pos]):\n",
        "      if syn.definition():\n",
        "        return syn.definition().split(\";\")[0]\n",
        "  return word\n",
        "      \n",
        "def antonimize_text(text):\n",
        "  tokens = nlp(text)\n",
        "  antonimized_words = []\n",
        "  for token in tokens:\n",
        "    antonimized_words.append(rec_find_antonym(token.text, token.pos_, depth))\n",
        "  return \" \".join(antonimized_words)\n",
        "\n",
        "print(antonimize_text(\"A simple truth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKYeZgfUs4x1"
      },
      "source": [
        "Reverse definition lookup\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U8JMncLtUKl",
        "outputId": "4976745e-95cc-46e3-b19d-017c5fd20948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "api = \"https://api.datamuse.com/words?ml=\"\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "def get_word_from_def(text):\n",
        "  api_url = api+\"+\".join(text.split())\n",
        "  response = requests.get(api_url)\n",
        "  if len(response.json())>0:\n",
        "    #print (response.json())\n",
        "    return response.json()[0][\"word\"]\n",
        "  else:\n",
        "    return text\n",
        "  \n",
        "print(get_word_from_def(\"capital of the united kingdom\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "london\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o1NcmSlwCIz"
      },
      "source": [
        "\n",
        "new antonymiser\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adDHqS2IveWF"
      },
      "source": [
        "def rec_find_antonym(word, pos, depth):\n",
        "        antonym = find_antonym(word, pos)\n",
        "        if antonym != word:\n",
        "          return antonym\n",
        "        if depth<=1:\n",
        "            return word\n",
        "        else:\n",
        "            definition = nlp(stem_word(find_definition(word, pos)))\n",
        "            return \" \".join([rec_find_antonym(token.text, token.pos_, depth-1) for token in definition])\n",
        "          \n",
        "def find_definition(word, pos):\n",
        "  if pos in conv_pos:\n",
        "    for syn in wordnet.synsets(word, pos=conv_pos[pos]):\n",
        "      if syn.definition():\n",
        "        return syn.definition().split(\";\")[0]\n",
        "  return word\n",
        "      \n",
        "def antonimize_text(text, depth=3):\n",
        "  tokens = nlp(text)\n",
        "  antonimized_words = []\n",
        "  for token in tokens:\n",
        "    antonym_def = rec_find_antonym(token.text, token.pos_, depth)\n",
        "    if len(antonym_def.split()) > 1:\n",
        "       antonym_def=get_word_from_def(antonym_def)\n",
        "    antonimized_words.append(antonym_def)\n",
        "  return \" \".join(antonimized_words)\n",
        "\n",
        "print(antonimize_text(\"foot\", 2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCgQ-J2yv9dP"
      },
      "source": [
        "word embeddings\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_msgSCNyhi0",
        "outputId": "d7ad41c0-e702-43a3-e811-7112354794b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "\n",
        "nlp_vec = spacy.load(\"en_core_web_lg\")  \n",
        "\n",
        "def semantic_similarity(word1, word2):\n",
        "  token1 = nlp_vec(word1)\n",
        "  token2 = nlp_vec(word2)\n",
        "  return token1.similarity(token2)\n",
        "\n",
        "print (semantic_similarity(\"life\", \"death\"))\n",
        "print (semantic_similarity(\"cat\", \"love\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "0.5892018368228293\n",
            "0.3469409206151455\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GlIPGjV0ghZ"
      },
      "source": [
        "#!wget -c http://mattmahoney.net/dc/enwik9.zip -P data\n",
        "#!unzip data/enwik9.zip -d data\n",
        "!pip install -U fasttext\n",
        "import fasttext\n",
        "!perl wikifil.pl data/enwik9 > data/fil9\n",
        "model = fasttext.train_unsupervised('data/fil9')\n",
        "#model = FastText('lid.176.ftz')\n",
        "#model.cbow(input='shelley.txt', output='model', epoch=100, lr=0.7)\n",
        "\n",
        "print(model.get_analogies(\"berlin\", \"germany\", \"france\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCCmuHH0ztFT"
      },
      "source": [
        "Hypernyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4iCBqYbzsta"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "def get_word_hyponyms(word):\n",
        "  word_synset = wn.synsets(word)[0]\n",
        "  return set([i.lemma_names()[0] \n",
        "              for i in word_synset.closure(lambda s:s.hyponyms())])\n",
        "\n",
        "def get_word_hypernyms(word, depth=4):\n",
        "  word_synset = wn.synsets(word)[0]\n",
        "  return set([i.lemma_names()[0] for i in word_synset.closure(lambda s:s.hypernyms(), depth)])\n",
        "  \n",
        "def get_word_cousins(word, depth_hyper=4):\n",
        "  hypernyms = get_word_hypernyms(word, depth_hyper)\n",
        "  cousins = set.union(*map(set, [get_word_hyponyms(item) for item in hypernyms])).union(hypernyms)\n",
        "  return cousins\n",
        "\n",
        "get_word_cousins(\"country\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}